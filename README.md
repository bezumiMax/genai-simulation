# genai-simulation

Пользователь в файле talk_user.py выбирает 3 параметра: РАЗМЕР СЕТКИ, КОЛ-ВО ЗЕЛЕНЫХ КВАДРАТОВ, СКОЛЬКО РАЗ АГЕНТ ПРОХОДИТ ОДИН И ТОТ ЖЕ УРОВЕНЬ.

Всего 3 уровня. Агент побеждает, если среднее значение награды по итогу прохождения 3 уровня > 0, иначе побеждает генератор.

Пользователь научится понимать, насколько быстро обучается генератор относительно агента и увидит, как они взаимодействуют между собой. А также поймет как хаотичность среды влияет на исход.

ПЕРВЫЙ ВАРИАНТ ИГРЫ:
Агент - double Q-learning, генератор - GANs

Параметры, при которых ВЫИГРЫВАЕТ АГЕНТ:
1. Маленький размер сетки (5×5)
Почему: 
    Меньше состояний (25 vs 81 в 9×9), значит табличный Q-learningбыстрее сходится
    Агенту нужно исследовать меньше пространства
    Меньше возможных комбинаций расположения квадратов

2. Мало зеленых квадратов (ближе к 1)
Почему:
    Задача проще: нужно найти всего 1-2 зеленых квадрата
    Меньше возможностей ошибиться
    Быстрее достигается успех (нет необходимости собирать много целей)
    На 3-м уровне: n_green + level, значит при маленьком начальном n_green задача остается выполнимой

3. Большое количество повторений уровня (rerun = 10)
Почему:
    Агент получает больше эпизодов: 450 * (rerun + 1) = 4950 при rerun=10
    Может дообучаться на одном и том же уровне
    Генератор не успевает адаптироваться (генерирует тот же уровень много раз)
    Агент успевает найти оптимальную стратегию для конкретной конфигурации


Параметры, при которых ВЫИГРЫВАЕТ ГЕНЕРАТОР:
1. Большой размер сетки (9×9)
Почему:
    81 состояние vs 25 (в 3+ раза больше)
    Табличный Q-learning медленнее сходится
    Больше пространства для "ловушек" от генератора
    Агенту сложнее исследовать все состояние

2. Много зеленых квадратов (максимальное значение)
Почему:
    На 3-м уровне: если начальное n_green большое, то на 3-м уровне будет n_green + 2
    Например: при сетке 9×9 максимум зеленых = (81 - 8) // 3 = 24
    На 3-м уровне: 26 зеленых квадратов - очень сложная задача
    Агенту нужно собрать много целей, не попадая на красные
    Красных всегда примерно в 2 раза больше: 2 * n_green ± 1

3. Мало или ноль повторений уровня (rerun = 0-2)
    Почему:
    Агент получает мало эпизодов: 450 * (0+1) = 450 при rerun=0
    Не успевает адаптироваться к уровню
    Генератор быстро меняет уровни (каждый раз новый)
    Агент не может дообучаться на успешных стратегиях


Для победы агента:
Размер сетки: 5
Зеленых квадратов: 1-2  
Rerun: 8-10
→ Маленькое пространство, простая цель, много тренировки

Для победы генератора:
Размер сетки: 9
Зеленых квадратов: 8+ (ближе к максимуму)
Rerun: 0-2



Суть игры: Отличная демонстрация trade-off между сложностью среды и возможностями обучения агента




ВТОРАЯ ВЕРСИЯ ИГРЫ:
Агент - генетический алгоритм (нейроэволюция), генратор - GANs


Параметры, при которых ВЫИГРЫВАЕТ АГЕНТ (ГА):
1. Средний размер сетки (7×7)
Почему:
    ГА плохо масштабируется на очень большие пространства (9×9 = 81 состояние)
    На слишком маленькой сетке (5×5) ГА избыточен, проще методы работают лучше
    7×7 = 49 состояний - оптимальный баланс для эволюционного поиска
    Достаточно пространства для разнообразия в популяции

2. Умеренное количество зеленых квадратов (3-5)
Почему:
    ГА хорошо решает задачи средней сложности с ясной фитнес-функцией
    Слишком мало целей (1-2) - перебор избыточен
    Слишком много целей (8+) - фитнес-функция становится шумной
    3-5 целей дают четкий градиент улучшения для отбора

3. Среднее количество повторений уровня (rerun = 4-6)
Почему:
    ГА требует времени для эволюции (поколений)
    Мало повторений → мало поколений → не успевает сойтись
    Слишком много повторений → переобучение на конкретный уровень
    4-6 дают ~10-15 поколений на уровень, что оптимально для ГА


Параметры, при которых ВЫИГРЫВАЕТ ГЕНЕРАТОР:
1. Очень большой или очень маленький размер сетки
Почему:
    9×9: 81 состояние → ГА требует огромной популяции и многих поколений
    5×5: 25 состояний → пространство слишком простое, ГА страдает от избыточного поиска
    В обоих случаях ГА неэффективен по сравнению с градиентными методами

2. Крайние значения зеленых квадратов (1 или максимум)
Почему:
    1 зеленый квадрат: Задача тривиальна, но ГА тратит время на избыточный поиск
    Максимум зеленых: Фитнес-функция шумная, сложно отличить хорошие особи от плохих
    ГА лучше всего работает на задачах с плавным ландшафтом приспособленности

3. Мало повторений уровня (rerun = 0-2)
Почему:
    ГА медленно стартует - нужны поколения для накопления хороших генов
    1-3 повторения = 2-4 поколения → недостаточно для эволюции
    Генератор успевает адаптироваться быстрее, чем ГА находит решение

4. Очень много повторений (rerun = 9-10)
Почему:
    ГА может переобучиться на конкретную конфигурацию
    Пропадает обобщающая способность
    Генератор, видя переобучение, может создать контринтуитивные уровни


Для победы агента (ГА):
Размер сетки: 7
Зеленых квадратов: 4
Rerun: 5
→ Средняя сложность, оптимальное количество поколений, четкая фитнес-функция

Для победы генератора:

    Вариант 1 (против переобучения):
    Размер сетки: 5
    Зеленых квадратов: 7-8 (ближе к максимуму для 5×5)
    Rerun: 9-10

    Вариант 2 (против недостатка обучения):
    Размер сетки: 9  
    Зеленых квадратов: 3-4
    Rerun: 1-2




Суть игры: разные алгоритмы RL имеют разные ниши эффективности